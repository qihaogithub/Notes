## 打标
标注(tag)就是通过一个个关键词将图片拆分为AI能理解的内容数据，进而让AI把这些tag和图片对应的内容特征绑定起来，后续我们在出图的时候，只要输入对应的特征tag关键词，AI就会明白我们想要的是什么样的图片。
![](https://pic3.zhimg.com/80/v2-3733efeea39716802dd4023681629bde_1440w.webp)
_**

### 触发词
触发词不能是通用词语，应该是 ai 不知道是什么的词。

### 固有特征
我们想要保留图片中的某些特征内容时，在打标的时候就要删除与之对应的标签
想要图片中某些内容随机时，在打标的时候就要添加与之对应的标签

## 文件夹准备
准备一个专门用来训练的文件夹目录，并在目录中建立四个子目录：image、reg、log、model

image目录中建立一个子目录如下：
![](https://pic1.zhimg.com/80/v2-2b6b1064241a4584eb4ebd7b8c8ec0f4_1440w.webp)

### Image
Image文件夹中存放训练图片及其提示词，内部文件夹命名规则为
n_identifierN class  ,分别对应重复次数、标识符、类别名

Image 中可以放多个文件夹，为每个文件夹设置不同的重复次数，不同的重复次数用来给不同的文件夹不同的权重。

> 例如，你分别设置了一个包含 20 个图像的 200_ HighRes （高分辨率的意思）文件夹和一个包含 10 个图像的 100_ LowRes （低分辨率的意思）文件夹。这样一来，一共有 5000 步数（20 * 200 + 10 * 100）的训练步数，那么程序在第一个文件夹中学习的时间是 80% (20 * 200/5000) ，在第二个文件夹中学习的时间是20% (10 * 100/5000) ，从而减少了低质量图片对训练的过分影响

identifiertN 文件夹可以有很多，即 N 的值可以很大，但至少要有一个

identifierN是训练对象的特有名称，即模型名称独有的标识符，也是 image 中的子文件夹名，必须为英文，且为非通用词汇，如：uhu 。最好为 3 个字母组成的罕见词，如：ici、idi、uku、oro、hru 等等，确定之前建议多查查字典，看看是否有一样的单词。“n”和 identifierA、identifierB…identifierN 即 identifier之间必须用下划线`_`链接；

class为通用的类别名，比如 dog、cat 之类。 `identifier`与 `class` 之间用空格链接，比如：100_uhu dog 。那么在图像生成阶段时，使用“uhu dog”作为提示词中的触发词，将生成所训练的 LoRA 模型的狗种类图像。虽然可以在一个 LoRA 模型训练中包含多个特征和风格，但我并不建议这样，最好是专项专用，即一个LoRA只训练一种特征或形象；

n_identifierN class 的每个文件夹内必须有图像 *.png 或其他格式的图像文件（图像文件格式支持 .png、.jpg、 .jpeg、 .webp、.bmp ）。如果存储了这些图片对应的描述文件 *.txt 或 *.caption（这类描述图片的文本文件称为：caption）则该文件夹名称中的 `identifier` `class` (原本是作为训练时使用的 token）便在模型训练时失效，系统改为读取 caption 中的文本描述内容。（注意：.txt 为扩展名的 caption 与 DeepDanbooru 会产生冲突）

文本描述文件除扩展名外，他们的名称必须与对应图片的名称一致。caption 中的描述文本必须有对应图片的实际 tag 描述或自然语言的文本描述，文本应该为一行，即不要手动回车换行。第一个被逗号分割的词，设置为训练对象的特有名称，即模型名称独有的标识符，即 Trigger Word 触发词，必须为英文，且为非通用词汇，如：uhu ；

### Reg
reg 代表 regularization images ，为正则化训练集所在文件夹。 reg 文件夹可以没有，它是一个可选项。但在理论上，正则化图像有助于提高训练精度。一旦设置 reg 文件夹，就应该放置相关的正则化图像，则训练用图像的“n1”需要满足（n1×图像的数量）≥（正则化图像的 n2 ×正则化图像的数量），通常，数百张正则化图像是理想的。正则化图像的子文件夹命名规则为： <num_repeats>_<class> 。正则化图像无需 caption。 (通常一 epoch 即一“轮”的总训练步数是：重复次数 n1 x 训练图像的数量。如果正则化图像的数目大于这个数目，则系统训练时不使用多余的正则化图像。)



