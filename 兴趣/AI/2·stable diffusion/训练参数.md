[【AI绘画】LoRA训练全参数讲解 - 哔哩哔哩](https://www.bilibili.com/read/cv22022392)

[Stable Diffusion Lora locon loha训练参数设置 - 知乎](https://zhuanlan.zhihu.com/p/618758020?utm_id=0)

lora 参数：

可调节的参数
[[batch_size]] · [[base_model]] · [[clip_skip]] · [[network_dimension]] · [[unet_lr]] · [[shuffle_captions]] · [[keep_tokens]]

默认即可的参数
[[learning_rate]] · [[alpha]] · [[steps]] · [[text_encoder_lr]] · [[scheduler]] · [[cycle]] · [[optimizer_type]] · [[unet_only  text_only]] · [[noise_offset]] · [[persistent_data_loader_workers]]






### 18. max_data_loader_n_workers`

如果cpu很烂，设置成1或2

> 指定数据加载的进程数。 大量的进程会更快地加载数据并更有效地使用 GPU，但会消耗更多的主内存。 默认是"`8`或者`CPU并发执行线程数 - 1`，取小者"，所以如果主存没有空间或者GPU使用率大概在90%以上，就看那些数字和 `2` 或将其降低到大约 `1`。

### 19. mixed_precision

混合精度，推荐bf16，保存精度请与它一致。

### 20. prior_loss_weight

使用正则化时使用，该权重控制先验知识强度，默认为1。使用100张以上训练集有人推荐是5%-10%，不要设置更低了，那等于没有正则化，使用正则化权重1时模型收敛会非常困难。我在设置为1时多次炸炉，推荐训练画风时使用正则化。

### 21. gradient_checkpointing

> 通过逐步计算权重而不是在训练期间一次计算所有权重来减少训练所需的 GPU 内存量。 关闭它不会影响准确性，但打开它允许更大的批量大小，所以那里有影响。  
> 另外，打开它通常会减慢速度，但可以增加批量大小，因此总的学习时间实际上可能会更快。

一个节约显存的设置，然而我看见有的训练脚本里内容是pass（根本不起作用）推荐关闭，除非你显存严重不足。

### 22. --xformers` / `--mem_eff_attn`

> 当指定 xformers 选项时，使用 xformers 的 CrossAttention。如果未安装 xformers 或发生错误（取决于环境，例如 `mixed_precision="no"`），请指定 `mem_eff_attn` 选项而不是使用 CrossAttention 的内存节省版本（比 xformers 慢）。

都不开启的情况，24GB显存也非常容易炸。通常开启xformers，不要开启 mem_eff_attn，速度感人。除非你是16系显卡……

### 23. cache_latents

缓存潜变量，通常开启，提高速度，与 random_crop 互斥。

> 此外，在学习过程中执行 `random_crop` 或 `color_aug` 时，无法提前获取 latents（因为每次学习时图像都会改变）。如果你不预取，你可以从到目前为止的元数据中学习。事先获取图像的潜在表示并将其保存到磁盘。这允许快速学习。同时进行bucketing（根据纵横比对训练数据进行分类）

### 24. enable_bucket

开启后会进行分桶，如果你的训练集有不同分辨率图像则开启，会略微多消耗显存。

### 25. flip_aug

> 如果指定 --flip_aug 选项，它将执行水平翻转扩充（数据扩充）。你可以人为的将数据量加倍，但是如果在数据不对称的时候指定（比如人物外貌、发型等），学习就不会很顺利。

可选项，如果学习画风可以开启，人物也可以考虑，但是要是人物刘海有方向这种，就会出事。默认关闭。

### 26. lr_warmup_steps

可开可不开的东西，默认10%。预热阶段，不要听信别人说什么开始需要学习率大，那是稳定了才需要的，开始的梯度本身就很大，开启有助于减少神经元失活问题。

说得好，我决定设置为 0（X）

## 三、Locon 训练参数

### 1. 如何理解 locon （Conventional LoRA）

通过矩阵乘法降阶矩阵来降低参数的量。如 W = AB

W: (m,n)

A: (m,p)

B: (p, n)

[鸠山弥次郎：LoCon相对于LoRA的改进95 赞同 · 11 评论文章![](https://pic2.zhimg.com/equation_ipico.jpg)](https://zhuanlan.zhihu.com/p/612133434)

### 2. 作者推荐参数

作者的推荐：

> Include Conv layer implementation from LoCon  
> recommended settings  
> dim <= 64  
> alpha = 1 (or lower, like 0.3)

### 3. 实验参数

有的人 卷积 40 alpha 40 线性 40 alpha 40 仍然有不错效果（玄学调参）我没怎么练，不知道。

## 四、Loha 训练参数

### 1. 如何理解 loha

通过哈达马积进一步降低参数的量.[[2]](https://zhuanlan.zhihu.com/p/618758020?utm_id=0#ref_2)

![](https://pic2.zhimg.com/80/v2-4f6d520b964dbc47af30a99c390e3b19_1440w.webp)

> Use network_dim=0 or conv_dim=0 to disable linear/conv layer  
> LoHa doesn't support dropout yet.

### 2. 调参

-   recommended settings

-   dim <= 32
-   alpha = 1 (or lower)

  

> **WARNING: You are not supposed to use dim>64 in LoHa, which is over sqrt(original_dim) for almost all layer in SD**  
> **High dim with LoHa may cause unstable loss or just goes to NaN. If you want to use high dim LoHa, please use lower lr**  
> **WARNING-AGAIN: Use parameter-efficient algorithim in parameter-unefficient way is not a good idea**

经过实验，效果不佳，可能是LOHA不适合训练特征不太明确的画风（很容易出现解剖结构崩坏的情况）。但是LOGA在特征较为明显的画风上表现比LOCON更好。推荐卷积层与线性层alpha都为1，线性层dim 32，64，16，8，卷积层dim 16，8，4，这也是目前比较成功的模型普遍采用的参数。

此外，loha 模型较难收敛，训练 Loha 画风普遍在 20000 - 30000 步……（所以说为了一点模型大小值得吗，不过在一些训练集上确实表现更好，另一些则是灾难）一些扑克之类构图训练也在 8000 步以上，只看到一两个模型2800 左右步数。

enable_sample_prompt:
> 启用示例提示符：

sampler:
> 采样器：	 

noise_offset:
> 噪波偏移量：

num_epochs:

vae_batch_size:
> Vae_批处理_大小

train_batch_size:
> 训练_批处理_大小：

mixed_precision: fp16
> 混合精度:

save_precision:
> 保存精度:

save_n_epochs_type:
 
save_n_epochs_type_value:

save_model_as:
> 将模型另存为：

max_token_length:
> 最大令牌长度：



gradient_checkpointing:
> 梯度 _ 检查点:

gradient_accumulation_steps:
> 梯度累积步骤:

seed:




